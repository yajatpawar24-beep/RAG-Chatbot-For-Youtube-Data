{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ojf0sELZImt",
        "outputId": "bee246cf-b49a-4d0c-c9b5-8e9262096616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.15.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: azure-core in /usr/local/lib/python3.12/dist-packages (1.38.0)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from azure-core) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from azure-core) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.21.0->azure-core) (2026.1.4)\n",
            "Requirement already satisfied: pinecone in /usr/local/lib/python3.12/dist-packages (8.0.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2026.1.4)\n",
            "Requirement already satisfied: orjson>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from pinecone) (3.11.5)\n",
            "Requirement already satisfied: pinecone-plugin-assistant<4.0.0,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from pinecone) (3.0.1)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.1.0,>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.12/dist-packages (from pinecone) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.12/dist-packages (from pinecone) (2.5.0)\n",
            "Requirement already satisfied: packaging<25.0,>=24.2 in /usr/local/lib/python3.12/dist-packages (from pinecone-plugin-assistant<4.0.0,>=3.0.1->pinecone) (24.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from pinecone-plugin-assistant<4.0.0,>=3.0.1->pinecone) (2.32.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<4.0.0,>=3.0.1->pinecone) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<4.0.0,>=3.0.1->pinecone) (3.11)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install openai\n",
        "!pip install azure-core\n",
        "!pip install pinecone\n",
        "\n",
        "# Mount Google Drive to access files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import userdata to get API keys securely\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve API keys and endpoint from Colab userdata secrets\n",
        "Azure_api_key = userdata.get('AZURE_API')\n",
        "Pinecone_api_key = userdata.get('PINECONE')\n",
        "Azure_endpoint = userdata.get('AZURE_ENDPOINT')\n",
        "Chat_endpoint = userdata.get('CHAT_ENDPOINT')\n",
        "Chat_api = userdata.get('SUBSCRIPTION_KEY')"
      ],
      "metadata": {
        "id": "Arf2RBzTemKV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Necessary Libraries\n",
        "from openai import AzureOpenAI\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from uuid import uuid4\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "# Define model parameters\n",
        "model_name = \"text-embedding-3-small\"\n",
        "deployment = \"text-embedding-3-small\"\n",
        "api_version = \"2024-02-01\"\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    azure_endpoint = Azure_endpoint,\n",
        "    api_version = api_version,\n",
        "    api_key = Azure_api_key\n",
        ")\n",
        "\n",
        "pc = Pinecone(api_key=Pinecone_api_key)\n",
        "\n",
        "pc.create_index(\n",
        "    name='youtube-rag-data',\n",
        "    dimension=1536,\n",
        "    spec=ServerlessSpec(cloud='aws',region='us-east-1')\n",
        ")\n",
        "\n",
        "print(pc.list_indexes())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrK1rc24erVD",
        "outputId": "d9f5cb69-067e-49c2-d5c9-6f5a3990d080"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{\n",
            "    \"name\": \"dotproduct-index\",\n",
            "    \"metric\": \"dotproduct\",\n",
            "    \"host\": \"dotproduct-index-nlsfoxv.svc.aped-4627-b74a.pinecone.io\",\n",
            "    \"spec\": {\n",
            "        \"serverless\": {\n",
            "            \"region\": \"us-east-1\",\n",
            "            \"cloud\": \"aws\",\n",
            "            \"read_capacity\": {\n",
            "                \"mode\": \"OnDemand\",\n",
            "                \"status\": {\n",
            "                    \"state\": \"Ready\",\n",
            "                    \"current_shards\": null,\n",
            "                    \"current_replicas\": null\n",
            "                }\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    \"status\": {\n",
            "        \"ready\": true,\n",
            "        \"state\": \"Ready\"\n",
            "    },\n",
            "    \"vector_type\": \"dense\",\n",
            "    \"dimension\": 1536,\n",
            "    \"deletion_protection\": \"disabled\",\n",
            "    \"tags\": null\n",
            "}, {\n",
            "    \"name\": \"practice\",\n",
            "    \"metric\": \"cosine\",\n",
            "    \"host\": \"practice-nlsfoxv.svc.aped-4627-b74a.pinecone.io\",\n",
            "    \"spec\": {\n",
            "        \"serverless\": {\n",
            "            \"region\": \"us-east-1\",\n",
            "            \"cloud\": \"aws\",\n",
            "            \"read_capacity\": {\n",
            "                \"mode\": \"OnDemand\",\n",
            "                \"status\": {\n",
            "                    \"state\": \"Ready\",\n",
            "                    \"current_shards\": null,\n",
            "                    \"current_replicas\": null\n",
            "                }\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    \"status\": {\n",
            "        \"ready\": true,\n",
            "        \"state\": \"Ready\"\n",
            "    },\n",
            "    \"vector_type\": \"dense\",\n",
            "    \"dimension\": 1536,\n",
            "    \"deletion_protection\": \"disabled\",\n",
            "    \"tags\": null\n",
            "}, {\n",
            "    \"name\": \"youtube-rag-data\",\n",
            "    \"metric\": \"cosine\",\n",
            "    \"host\": \"youtube-rag-data-nlsfoxv.svc.aped-4627-b74a.pinecone.io\",\n",
            "    \"spec\": {\n",
            "        \"serverless\": {\n",
            "            \"region\": \"us-east-1\",\n",
            "            \"cloud\": \"aws\",\n",
            "            \"read_capacity\": {\n",
            "                \"mode\": \"OnDemand\",\n",
            "                \"status\": {\n",
            "                    \"state\": \"Ready\",\n",
            "                    \"current_shards\": null,\n",
            "                    \"current_replicas\": null\n",
            "                }\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    \"status\": {\n",
            "        \"ready\": true,\n",
            "        \"state\": \"Ready\"\n",
            "    },\n",
            "    \"vector_type\": \"dense\",\n",
            "    \"dimension\": 1536,\n",
            "    \"deletion_protection\": \"disabled\",\n",
            "    \"tags\": null\n",
            "}, {\n",
            "    \"name\": \"semantic-search\",\n",
            "    \"metric\": \"cosine\",\n",
            "    \"host\": \"semantic-search-nlsfoxv.svc.aped-4627-b74a.pinecone.io\",\n",
            "    \"spec\": {\n",
            "        \"serverless\": {\n",
            "            \"region\": \"us-east-1\",\n",
            "            \"cloud\": \"aws\",\n",
            "            \"read_capacity\": {\n",
            "                \"mode\": \"OnDemand\",\n",
            "                \"status\": {\n",
            "                    \"state\": \"Ready\",\n",
            "                    \"current_shards\": null,\n",
            "                    \"current_replicas\": null\n",
            "                }\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    \"status\": {\n",
            "        \"ready\": true,\n",
            "        \"state\": \"Ready\"\n",
            "    },\n",
            "    \"vector_type\": \"dense\",\n",
            "    \"dimension\": 1536,\n",
            "    \"deletion_protection\": \"disabled\",\n",
            "    \"tags\": null\n",
            "}, {\n",
            "    \"name\": \"datacamp-index\",\n",
            "    \"metric\": \"cosine\",\n",
            "    \"host\": \"datacamp-index-nlsfoxv.svc.aped-4627-b74a.pinecone.io\",\n",
            "    \"spec\": {\n",
            "        \"serverless\": {\n",
            "            \"region\": \"us-east-1\",\n",
            "            \"cloud\": \"aws\",\n",
            "            \"read_capacity\": {\n",
            "                \"mode\": \"OnDemand\",\n",
            "                \"status\": {\n",
            "                    \"state\": \"Ready\",\n",
            "                    \"current_shards\": null,\n",
            "                    \"current_replicas\": null\n",
            "                }\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    \"status\": {\n",
            "        \"ready\": true,\n",
            "        \"state\": \"Ready\"\n",
            "    },\n",
            "    \"vector_type\": \"dense\",\n",
            "    \"dimension\": 1536,\n",
            "    \"deletion_protection\": \"disabled\",\n",
            "    \"tags\": null\n",
            "}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = pc.Index('youtube-rag-data')"
      ],
      "metadata": {
        "id": "5fA7dPg3fh8Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/youtube_rag_data.csv')\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "0MrKG6umfxdU",
        "outputId": "60b7c943-cc96-4dd0-fdc1-d3e38fb34d93"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                id  \\\n",
              "0                 35Pdoyi6ZoQ-t0.0   \n",
              "1               35Pdoyi6ZoQ-t18.48   \n",
              "2               35Pdoyi6ZoQ-t32.36   \n",
              "3  35Pdoyi6ZoQ-t51.519999999999996   \n",
              "4               35Pdoyi6ZoQ-t67.28   \n",
              "\n",
              "                                                blob  \\\n",
              "0  {'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...   \n",
              "1  {'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...   \n",
              "2  {'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...   \n",
              "3  {'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...   \n",
              "4  {'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...   \n",
              "\n",
              "                 channel_id  end                published  start  \\\n",
              "0  UCv83tO5cePwHMt1952IVVHw   74  2021-07-06 13:00:03 UTC      0   \n",
              "1  UCv83tO5cePwHMt1952IVVHw   94  2021-07-06 13:00:03 UTC     18   \n",
              "2  UCv83tO5cePwHMt1952IVVHw  108  2021-07-06 13:00:03 UTC     32   \n",
              "3  UCv83tO5cePwHMt1952IVVHw  125  2021-07-06 13:00:03 UTC     51   \n",
              "4  UCv83tO5cePwHMt1952IVVHw  140  2021-07-06 13:00:03 UTC     67   \n",
              "\n",
              "                                                text  \\\n",
              "0  Hi, welcome to the video. So this is the fourt...   \n",
              "1  So we got some data. We built a tokenizer with...   \n",
              "2  So let's move over to the code. And we see her...   \n",
              "3  PyTorch data loader, ready. And we can begin t...   \n",
              "4  So when we're training a model for mass langua...   \n",
              "\n",
              "                                               title  \\\n",
              "0  Training and Testing an Italian BERT - Transfo...   \n",
              "1  Training and Testing an Italian BERT - Transfo...   \n",
              "2  Training and Testing an Italian BERT - Transfo...   \n",
              "3  Training and Testing an Italian BERT - Transfo...   \n",
              "4  Training and Testing an Italian BERT - Transfo...   \n",
              "\n",
              "                            url  \n",
              "0  https://youtu.be/35Pdoyi6ZoQ  \n",
              "1  https://youtu.be/35Pdoyi6ZoQ  \n",
              "2  https://youtu.be/35Pdoyi6ZoQ  \n",
              "3  https://youtu.be/35Pdoyi6ZoQ  \n",
              "4  https://youtu.be/35Pdoyi6ZoQ  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b1d97073-f0f0-43bb-b697-4ccbcecc7747\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>blob</th>\n",
              "      <th>channel_id</th>\n",
              "      <th>end</th>\n",
              "      <th>published</th>\n",
              "      <th>start</th>\n",
              "      <th>text</th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>35Pdoyi6ZoQ-t0.0</td>\n",
              "      <td>{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>74</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>0</td>\n",
              "      <td>Hi, welcome to the video. So this is the fourt...</td>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>35Pdoyi6ZoQ-t18.48</td>\n",
              "      <td>{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>94</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>18</td>\n",
              "      <td>So we got some data. We built a tokenizer with...</td>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>35Pdoyi6ZoQ-t32.36</td>\n",
              "      <td>{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>108</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>32</td>\n",
              "      <td>So let's move over to the code. And we see her...</td>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>35Pdoyi6ZoQ-t51.519999999999996</td>\n",
              "      <td>{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>125</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>51</td>\n",
              "      <td>PyTorch data loader, ready. And we can begin t...</td>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>35Pdoyi6ZoQ-t67.28</td>\n",
              "      <td>{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'en...</td>\n",
              "      <td>UCv83tO5cePwHMt1952IVVHw</td>\n",
              "      <td>140</td>\n",
              "      <td>2021-07-06 13:00:03 UTC</td>\n",
              "      <td>67</td>\n",
              "      <td>So when we're training a model for mass langua...</td>\n",
              "      <td>Training and Testing an Italian BERT - Transfo...</td>\n",
              "      <td>https://youtu.be/35Pdoyi6ZoQ</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b1d97073-f0f0-43bb-b697-4ccbcecc7747')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b1d97073-f0f0-43bb-b697-4ccbcecc7747 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b1d97073-f0f0-43bb-b697-4ccbcecc7747');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2285,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2285,\n        \"samples\": [\n          \"I3na13AESjw-t1711.0\",\n          \"3IPCEeh4xTg-t1620.8000000000002\",\n          \"x1lAcT3xl5M-t542.0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"blob\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2285,\n        \"samples\": [\n          \"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1840, 'published': '2022-07-13 16:22:08 UTC', 'start': 1711, 'text': \\\"Now there was further work on color histograms to improve some of these. So for example different techniques that consider the texture of images. They consider the edges within the images and a few other things. But still all of these things they're not going to get you as far as some of the more recent deep learning methods that allow you to actually consider what is inside the image from a very human perspective. But that being said if you just want to return images or retrieve images that have a similar sort of aesthetic to a particular image that you're searching with. So you want a similar color profile like we saw with the earlier results up here. We are returning pictures that kind of look like the first image like the query image. So that's one pro to using this technique. Another one is that it's incredibly easy to implement. We've just done it and in reality we don't even need to go through that sort of slow building a histogram part. We can just do it super quickly using OpenCV. And another key benefit is the results are very interpretable. So with like a lot of deep learning methods you have a black box. You put in some data. You return some results. Why did you get those results? A lot of the time you don't really know. With this you know exactly why you're returning a particular result. You can see that the color profile is very similar to the one the particular color profile that you're querying with. So we understand what is actually happening here. It's not a black box like neural networks.\\\", 'title': 'How to use Color Histograms for Image Retrieval', 'url': 'https://youtu.be/I3na13AESjw'}\",\n          \"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 1804, 'published': '2021-12-17 14:24:40 UTC', 'start': 1620, 'text': \\\"And this would probably have a similar effect to having such a significant number of warm up sets. And then in this case you could decrease this to 1 or 10 percent. But for me the way I've tested this I've ended up going with 40 percent warm up sets and that works quite well. So the final step here is where do we want to save our model. So I'm going to say I want to save it into BERT base cross encoder or let's say BERT STSB cross encoder. And we can run that and that will run everything for us. I'll just make sure it's actually. Yep there we go. So see it's running but I'm not going to run it because I've already done it. So let me pause that and I will move on to the next step. OK. So we now have our gold data set which we have pulled from HuginFace data sets and we've just fine tuned a cross encoder. So let's cross both of those off of here. This and this. And now so before we actually go on to predicting labels with the cross encoder we need to actually create that unlabeled data set. So let's do that through random sampling using the gold data set you already have. And then we can move on to the next steps. OK. So I'll just add a little bit of separation in here. So now we're going to go ahead and create the augmented data. So as I said we're going to be using random sampling for that. And I find that the easiest way to do that is to actually go ahead and use a Pandas data frame rather than using the HuginFace data set object that we currently have. So I'm going to go ahead and initialize that. So we have our gold data. That will be pde.data frame. And in here we're going to have sentence one and sentence two. So sentence one. That is going to be equal to stsb sentence one. OK. And as well as that we also have sentence two which is going to be stsb sentence two. Now we may also want to include our\\\", 'title': 'Making The Most of Data: Augmented SBERT', 'url': 'https://youtu.be/3IPCEeh4xTg'}\",\n          \"{'channel_id': 'UCv83tO5cePwHMt1952IVVHw', 'end': 586, 'published': '2021-05-27 16:15:39 UTC', 'start': 542, 'text': \\\"we would want to do something like that but for this use case I don't want to get make things too complicated. So the reason I'm doing that is for example this sentence is just a single sentence in that paragraph and we can't guarantee that each continuous paragraph is talking about the same subject you might switch. So for the sake of simplicity I'm just going to ignore the single sentence paragraphs although we do have them in our bag so they can be pulled in as potential sentence bees when we randomise the selection. Now what we want to do is set the sentence that we will\\\", 'title': 'Training BERT #4 - Train With Next Sentence Prediction (NSP)', 'url': 'https://youtu.be/x1lAcT3xl5M'}\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"channel_id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"UCv83tO5cePwHMt1952IVVHw\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"end\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 653,\n        \"min\": 46,\n        \"max\": 4232,\n        \"num_unique_values\": 1365,\n        \"samples\": [\n          1109\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"published\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"2021-08-20 16:00:16 UTC\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"start\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 648,\n        \"min\": 0,\n        \"max\": 4047,\n        \"num_unique_values\": 1349,\n        \"samples\": [\n          1232\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2285,\n        \"samples\": [\n          \"Now there was further work on color histograms to improve some of these. So for example different techniques that consider the texture of images. They consider the edges within the images and a few other things. But still all of these things they're not going to get you as far as some of the more recent deep learning methods that allow you to actually consider what is inside the image from a very human perspective. But that being said if you just want to return images or retrieve images that have a similar sort of aesthetic to a particular image that you're searching with. So you want a similar color profile like we saw with the earlier results up here. We are returning pictures that kind of look like the first image like the query image. So that's one pro to using this technique. Another one is that it's incredibly easy to implement. We've just done it and in reality we don't even need to go through that sort of slow building a histogram part. We can just do it super quickly using OpenCV. And another key benefit is the results are very interpretable. So with like a lot of deep learning methods you have a black box. You put in some data. You return some results. Why did you get those results? A lot of the time you don't really know. With this you know exactly why you're returning a particular result. You can see that the color profile is very similar to the one the particular color profile that you're querying with. So we understand what is actually happening here. It's not a black box like neural networks.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"Locality Sensitive Hashing (LSH) for Search with Shingling + MinHashing (Python)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 50,\n        \"samples\": [\n          \"https://youtu.be/e_SBq3s20M8\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_limit = 100\n",
        "\n",
        "for batch in np.array_split(df, len(df)/batch_limit):\n",
        "  metadatas = [\n",
        "      {\n",
        "          'text_id' : row['id'],\n",
        "          'text' : row['text'],\n",
        "          'title' : row['title'],\n",
        "          'url' : row['url'],\n",
        "          'published' : row['published']\n",
        "      }\n",
        "      for _, row in batch.iterrows()\n",
        "  ]\n",
        "\n",
        "  texts = batch['text'].tolist()\n",
        "  ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "\n",
        "  response = client.embeddings.create(\n",
        "      input=texts,\n",
        "      model=model_name\n",
        "  )\n",
        "\n",
        "  embeds = [np.array(x.embedding) for x in response.data]\n",
        "\n",
        "  index.upsert(\n",
        "      vectors = zip(ids, embeds, metadatas),\n",
        "      namespace = 'youtube-data'\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5N8H3Owfo-4",
        "outputId": "4ce1fbbc-99dd-4f0b-9e19-ff7c5e237d62"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(query, top_k, namespace, emb_model):\n",
        "  \"\"\"\n",
        "    Retrieve relevant documents for a query.\n",
        "\n",
        "    Args:\n",
        "        query: User's question\n",
        "        top_k: Number of documents to retrieve\n",
        "        namespace: Pinecone namespace to search\n",
        "        emb_model: Embedding model name\n",
        "\n",
        "    Returns:\n",
        "        retrieved_docs: List of document texts\n",
        "        sources: List of (title, url) tuples\n",
        "    \"\"\"\n",
        "\n",
        "  query_response = client.embeddings.create(\n",
        "        input = query,\n",
        "        model = emb_model\n",
        "    )\n",
        "\n",
        "  query_emb = query_response.data[0].embedding\n",
        "\n",
        "  docs = index.query(\n",
        "        vector = query_emb,\n",
        "        top_k = top_k,\n",
        "        namespace = namespace,\n",
        "        include_metadata = True\n",
        "    )\n",
        "\n",
        "  retrieved_docs = []\n",
        "  sources = []\n",
        "\n",
        "  for doc in docs['matches']:\n",
        "    retrieved_docs.append(doc['metadata']['text'])\n",
        "    sources.append((\n",
        "        doc['metadata']['title'],\n",
        "        doc['metadata']['url']\n",
        "    ))\n",
        "\n",
        "  return retrieved_docs, sources\n",
        "\n",
        "query = \"How to build next-level Q&A with OpenAI?\"\n",
        "documents, sources = retrieve(\n",
        "    query=query,\n",
        "    top_k = 3,\n",
        "    namespace = 'youtube-data',\n",
        "    emb_model = model_name\n",
        ")\n",
        "\n",
        "print(f\"Retrieved {len(documents)} documents\")\n",
        "for i, (doc, source) in enumerate(zip(documents, sources)):\n",
        "    print(f\"\\nDocument {i+1}:\")\n",
        "    print(f\"Source: {source[0]}, {source[1]}\")\n",
        "    print(f\"Text: {doc[:100]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk710Doqhp7g",
        "outputId": "77ddc899-75bf-41e3-b6e8-104586342546"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved 3 documents\n",
            "\n",
            "Document 1:\n",
            "Source: How to build a Q&A AI in Python (Open-domain Question-Answering), https://youtu.be/w1dMEWm7jBc\n",
            "Text: to use for Open Domain Question Answering. We're going to start with a few examples. Over here we ha...\n",
            "\n",
            "Document 2:\n",
            "Source: How to build a Q&A AI in Python (Open-domain Question-Answering), https://youtu.be/w1dMEWm7jBc\n",
            "Text: There are places where you do want to keep traditional search. But particularly for unstructured tex...\n",
            "\n",
            "Document 3:\n",
            "Source: How to build next-level Q&A with OpenAI, https://youtu.be/coaaSxys5so\n",
            "Text: So let's go with, let's restrict everything to streamlit and we'll ask about OpenAI Clip. Maybe. Let...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_with_context_builder(query,docs):\n",
        "\n",
        "  \"\"\"\n",
        "  Build a prompt with retrieved context.\n",
        "\n",
        "  Args:\n",
        "        query: User's question\n",
        "        docs: List of retrieved document texts\n",
        "\n",
        "  Returns:\n",
        "        Formatted prompt string\n",
        "  \"\"\"\n",
        "\n",
        "  delim = '\\n\\n--\\n\\n'\n",
        "  prompt_start = 'Answer the question based on the context below. \\n\\nContext:\\n'\n",
        "  prompt_end = f'\\n\\nQuestion: {query}\\nAnswer:'\n",
        "\n",
        "  prompt = prompt_start + delim.join(docs) + prompt_end\n",
        "  return prompt\n",
        "\n",
        "query = \"How to build next-level Q&A with OpenAI\"\n",
        "context_prompt = prompt_with_context_builder(query,documents)\n",
        "\n",
        "print(context_prompt[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvZg9igwlWNY",
        "outputId": "0924d345-b691-40e6-82fb-911260b3599a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the question based on the context below. \n",
            "\n",
            "Context:\n",
            "to use for Open Domain Question Answering. We're going to start with a few examples. Over here we have Google and we can ask Google questions like we would a normal person. So we can say, how do I tie my shoelaces? So what we have right here is three components to the question and answer. And I want you to remember these because these are relevant for what we are going to be building. We have the query at the top. We have what we can ref\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = Chat_endpoint\n",
        "model_name = \"gpt-5.2-chat\"\n",
        "deployment = \"gpt-5.2-chat-2\"\n",
        "\n",
        "subscription_key = Chat_api\n",
        "api_version = \"2024-12-01-preview\"\n",
        "\n",
        "client1 = AzureOpenAI(\n",
        "    api_version=api_version,\n",
        "    azure_endpoint=endpoint,\n",
        "    api_key=subscription_key,\n",
        ")\n",
        "\n",
        "def question_answering(prompt,sources,chat_model):\n",
        "  \"\"\"\n",
        "    Generate answer using LLM with retrieved context.\n",
        "\n",
        "    Args:\n",
        "        prompt: Prompt with context and question\n",
        "        sources: List of (title, url) tuples\n",
        "        chat_model: OpenAI model name\n",
        "\n",
        "    Returns:\n",
        "        Answer with source citations\n",
        "    \"\"\"\n",
        "\n",
        "  sys_prompt = \"You are a helpful assistant that always answers questions\"\n",
        "\n",
        "  res = client1.chat.completions.create(\n",
        "      model = chat_model,\n",
        "      messages = [\n",
        "          {'role':'system','content':sys_prompt},\n",
        "          {'role':'user','content':prompt}\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  answer = res.choices[0].message.content.strip()\n",
        "\n",
        "  answer += '\\n\\nSources:'\n",
        "  for source in sources:\n",
        "    answer += f'\\n{source[0]}:{source}[1]'\n",
        "  return answer\n",
        "\n",
        "\n",
        "query = \"How to build next-level Q&A with OpenAI\"\n",
        "answer = question_answering(\n",
        "    context_prompt,\n",
        "    sources,\n",
        "    chat_model=deployment\n",
        ")\n",
        "\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbY1YcNWm2dg",
        "outputId": "cd4f2eef-f82f-42be-becc-9c4a14cc56ca"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Answer:**\n",
            "\n",
            "To build next-level Q&A with OpenAI, you use an **Open Domain Question Answering (ODQA) pipeline**. This involves:\n",
            "\n",
            "1. **Taking a user query** and converting it into a vector using an **embedding (retrieval) model**.  \n",
            "2. **Retrieving relevant context** (text, video transcripts, audio, or other unstructured data) by comparing vector similarity, which captures semantic meaning rather than just keywords.  \n",
            "3. **Passing the retrieved context plus the question** to a **generative language model** (like OpenAI’s models).  \n",
            "4. **Prompting the model to answer strictly based on the provided context**, so it can give accurate answers or say “I don’t know” when the information isn’t present.\n",
            "\n",
            "This combination of semantic retrieval + generative answering enables powerful, intelligent Q&A systems beyond traditional keyword-based search.\n",
            "\n",
            "Sources:\n",
            "How to build a Q&A AI in Python (Open-domain Question-Answering):('How to build a Q&A AI in Python (Open-domain Question-Answering)', 'https://youtu.be/w1dMEWm7jBc')[1]\n",
            "How to build a Q&A AI in Python (Open-domain Question-Answering):('How to build a Q&A AI in Python (Open-domain Question-Answering)', 'https://youtu.be/w1dMEWm7jBc')[1]\n",
            "How to build next-level Q&A with OpenAI:('How to build next-level Q&A with OpenAI', 'https://youtu.be/coaaSxys5so')[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"text-embedding-3-small\"\n",
        "\n",
        "def rag_pipeline(query, top_k=3, namespace = 'youtube-data'):\n",
        "  \"\"\"\n",
        "    Complete RAG pipeline: retrieve → build prompt → generate answer.\n",
        "\n",
        "    Args:\n",
        "        query: User's question\n",
        "        top_k: Number of documents to retrieve\n",
        "        namespace: Pinecone namespace\n",
        "\n",
        "    Returns:\n",
        "        Generated answer with sources\n",
        "    \"\"\"\n",
        "\n",
        "  documents,sources = retrieve(\n",
        "      query,\n",
        "      top_k = top_k,\n",
        "      namespace = namespace,\n",
        "      emb_model = model_name\n",
        "  )\n",
        "\n",
        "  prompt_with_context = prompt_with_context_builder(query,documents)\n",
        "\n",
        "  answer = question_answering(\n",
        "      prompt_with_context,\n",
        "      sources,\n",
        "      chat_model = deployment\n",
        "  )\n",
        "\n",
        "  return answer\n",
        "\n",
        "query = \"How to build next-level Q&A with OpenAI?\"\n",
        "answer = rag_pipeline(query)\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xd0cgXSyBgjR",
        "outputId": "964fb6f9-4a98-4888-e2a3-6ed27f772d97"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To build next‑level Q&A with OpenAI, you use an **Open Domain Question Answering (ODQA) pipeline**. Based on the context, the key steps are:\n",
            "\n",
            "1. **Take a user query**  \n",
            "   Start with a natural‑language question.\n",
            "\n",
            "2. **Retrieve relevant context**  \n",
            "   - Convert the question into a vector using an **embedding model**.  \n",
            "   - Compare it against vectors of unstructured data (text, video transcripts, audio, docs) stored in a vector database.  \n",
            "   - Retrieve the most semantically relevant chunks, not just keyword matches.\n",
            "\n",
            "3. **Provide context to a generative model**  \n",
            "   - Pass the retrieved text as *context* along with the question.  \n",
            "   - Prompt the model to answer **only using the given context** (or say “I don’t know” if the answer isn’t present).\n",
            "\n",
            "4. **Generate the answer**  \n",
            "   - Use an OpenAI completion / generative model to produce a clear, natural answer grounded in that context.\n",
            "\n",
            "In short:  \n",
            "**Question → Embeddings → Semantic Retrieval → Context → OpenAI Generative Model → Answer**.\n",
            "\n",
            "Sources:\n",
            "How to build a Q&A AI in Python (Open-domain Question-Answering):('How to build a Q&A AI in Python (Open-domain Question-Answering)', 'https://youtu.be/w1dMEWm7jBc')[1]\n",
            "How to build a Q&A AI in Python (Open-domain Question-Answering):('How to build a Q&A AI in Python (Open-domain Question-Answering)', 'https://youtu.be/w1dMEWm7jBc')[1]\n",
            "How to build next-level Q&A with OpenAI:('How to build next-level Q&A with OpenAI', 'https://youtu.be/coaaSxys5so')[1]\n"
          ]
        }
      ]
    }
  ]
}